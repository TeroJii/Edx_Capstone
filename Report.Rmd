---
title: "Predicting the Chromatographic Retention Time of Small Molecules"
subtitle: "Capstone Project Report"
author: "Tero Jalkanen"
date: "01/2022"
output: 
  bookdown::html_document2:
    code_folding: show
#  bookdown::pdf_document2
bibliography: mybibfile.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r package_setup, include = FALSE}

## Library for plotting small molecule chemical structure
if(!require(ChemmineR)){
  if(!require(BiocManager))  install.packages("BiocManager", repos = "http://cran.us.r-project.org")
  BiocManager::install("ChemmineR", force = TRUE)}

## Data wrangling and visualization
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
        #### Includes: ggplot2 (data visualisation), dplyr (data manipulation), tidyr (data tidying), readr (data import), purrr (functional programming), tibble (tibbles, a modern re-imagining of data frames), stringr (strings), forcats (factors), readxl (.xls and .xlsx sheets), rvest (web scraping), lubridate (dates and date-times), broom (turning models into tidy data)
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org") # For multiple plots
#if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org") # For multiple plots
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org") # Visualizing correlation
# DiagrammeR only works for html output
#if(!require(DiagrammeR)) install.packages("DiagrammeR", repos = "http://cran.us.r-project.org") # Building diagrams with R

## Modelling
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidymodels)) install.packages("tidymodels", repos = "http://cran.us.r-project.org")
if(!require(MASS)) install.packages("MASS", repos = "http://cran.us.r-project.org")
if(!require(discrim)) install.packages("discrim", repos = "http://cran.us.r-project.org") #LDA and QDA
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org") # Random forest
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org") #Ridge and Lasso
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org") #CART
if(!require(vip)) install.packages("vip", repos = "http://cran.us.r-project.org") #variable importance for trees
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org") # boosted trees
#if(!require()) install.packages("", repos = "http://cran.us.r-project.org")

## Typography
if(!require(bookdown)) install.packages("bookdown", repos = "http://cran.us.r-project.org") #Fig. references etc.
#if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org") # Nicer table formatting
if(!require(equatiomatic)) install.packages("equatiomatic", repos = "http://cran.us.r-project.org") # automatic way to show model equations

## Set graphical theme
theme_set(theme_bw())
```


\newpage


```{r dataset}
#download modified data, the final dataset
df <- read.csv(file = "final_dataset.csv")

# One example molecule
molecule <- ChemmineR::pubchemCidToSDF(as.numeric(df$pubchem[3]))
```


# Introduction

Liquid chromatography (LC) is an analytical laboratory method widely used with organic small molecules. LC can be used for separating different molecules in a mixture, or in an analytical way, where the aim is to detect the presence or relative proportion of molecules in a mixture.

```{r molecule, fig.cap= "The chemical structure of one example molecule chosen from the original dataset.", fig.height=5, fig.width=5}

plot(molecule[1], print = FALSE, print_cid = "")

#rm(molecule)
```

In a LC measurement small molecules are dissolved in a fluid. The fluid with the small molecules (i.e. the analytes) is called the "mobile phase", and is washed through an LC column. The molecules in the mobile phase usually have an affinity towards the surface of the LC column, and are adsorbed or bound which slows them down, whereas the fluid carrying the molecules, the eluent, is washed through. Depending on the structure of the analyte molecule and the type of LC column, the molecules are retained in the column for differing amounts of time before exiting the column. This time is called the retention time, and it can be used, for example, in estimating the amounts of different molecules in a mixture.

The prediction of retention time is not a straightforward task. Among other thing the retention time depends on the type of LC column, and the structure of the molecule. Moreover, things such as the age of the column have an effect on the measured retention time. The METLIN small molecule dataset contains the retention times for over 80,000 molecules, measured with reverse-phase liquid chromatography [@Xavier2019a]. The dataset is freely available, and it was created for the purpose of enabling machine-learning based retention time prediction [@Xavier2019b]. Domingo-Almenara *et al.* have demonstrated the utility of this dataset by predicting the retention time with a deep-learning model utilizing the calculated molecular fingerprints of the small molecules [@Xavier2019a].

Here we will explore if using a set of simple molecular descriptors could be used for the same purpose.

## Structure of the data

The original dataset contains three columns, namely the measured retention time in seconds, the molecular structure of the small molecule in the form of InChI textual identifier (*[International Chemical Identifier](https://en.wikipedia.org/wiki/International_Chemical_Identifier)*), and the *[PubChem](https://en.wikipedia.org/wiki/PubChem)* identification number [@Xavier2019b]. Figure \@ref(fig:molecule) shows an example of one of the molecules in the dataset. These three columns alone do not give us much to work with. In the original paper, the authors used elaborate molecular fingerprints for retention time prediction. Here we aim for a differing approach, where as a preparatory task we have calculated several molecular descriptors for each molecule from the molecular structure. These desciptors, along with the retention time, will be used as our final dataset for this project.


### Distribution of retention time values

The distribution of retention times is shown in Figure \@ref(fig:hist1). The molecules which come through during the first 5 minutes or 300 seconds can be though as not having any specific affinity towards the LC column. They exit the column directly with the eluent. In this case, the retention time can not be used for laboratory analytical purposes.

```{r hist1, fig.cap= "Histogram of retention times with a) linear and b) logarithmic y-axis. The molecules which are not retained in the LC column can be seen below 300 s. They are basically washed out with eluent, and are not very useful for analytical purposes.", , fig.height=3}
#Histogram of the retention times
hist1 <- df %>% ggplot(aes(x = rt)) +
  geom_histogram(fill = "gray", color = "black") +
  ggtitle("a)") +
  labs(x = "Retention time (s)", y = "Number of molecules #") +
  geom_vline(xintercept = 300, color = "red", lty = 2) #not retained molecules below this line

#Histogram with logarithmic y-axis
hist2 <- df %>% 
  ggplot(aes(x = rt)) +
  geom_histogram(aes(fill = rt<300), color = "black") + # Fill color by retainment status
  ggtitle("b)") +
  labs(x = "Retention time (s)", y = "Number of molecules #") +
  geom_vline(xintercept = 300, color = "red", lty = 2) + #not retained molecules below this line
  scale_y_log10()

grid.arrange(hist1, 
             hist2, 
             ncol = 2,
             widths = c(2,3))

rm(hist1, hist2)
#df %>% filter((rt < 310 & rt > 280))
# Retained molecules with rt-values below 300 s ???
```



## Aims of the project

This project has two aims:

* First objective is to investigate if molecular descriptors such as physico-chemical properties can be used to accurately classify the molecules with regard to their retention class. That is whether a given molecule is retained or not retained in the LC column.

* The second objective is to try to predict the retention time for the retained molecules based on the molecular properties.

The first objective is interesting in the sense that it can help us identify molecules which are not suitable for a certain LC method. However, the main objective of this project is to try to estimate the retention time of retained small molecules.


# Methods

## Data preparation

As we discussed above, the original dataset contained only three columns [@Xavier2019b]. However, as a preparatory step, several molecular descriptors have been calculated for the molecules using two cheminformatics `R` packages, namely the [*ChemmineR*](https://www.bioconductor.org/packages/devel/bioc/vignettes/ChemmineR/inst/doc/ChemmineR.html#ChemmineR:_Cheminformatics_Toolkit_for_R) [@Cao2008] and [*Rcpi*](https://www.bioconductor.org/packages/devel/bioc/vignettes/Rcpi/inst/doc/Rcpi.html) [@Cao2014] packages. The code for creating the dataset with the molecular descriptors is provided in the GitHub repository in a script file called `CreateDataset.R`. The final dataset contains `r ncol(df)` columns and `r nrow(df)` observations.

## Model training & evaluation

Model training for the classification and regression tasks will utilize a test/train split. Moreover, $k$-fold cross-validation will be utilized in the training phase for some models. The re-sampling methodology is further elaborated in Figure \@ref(fig:datasplitting).


```{r datasplitting, fig.cap="Splitting of data for training and testing purposes. Training data is further split into $k$ roughly equally sized pieces in k-fold cross-validation.", fig.height=3, fig.align='center'}
# Code to create data splitting diagram
# output only works with html but not pdf
#
# DiagrammeR::grViz("digraph {
#   graph [layout = dot, rankdir = TB]
# 
#   node [shape = rectangle]
#   rec1 [label = 'Original data']
#   rec3 [label =  'Train data']
#   rec2 [label = 'Test data']
#   rec4 [label = 'Fold 1']
#   rec5 [label = 'Fold 2 ... Fold k-1']
#   rec6 [label = 'Fold k']
# 
#   # edge definitions with the node IDs
#   rec1 -> rec2
#   rec1 -> rec3 -> rec4
#   rec3 -> rec5
#   rec3 -> rec6
#   }",
#   height = 300)

knitr::include_graphics(path = "DataSplit.PNG")
```

### Metrics for classification

For the classification models we will use the area under the receiver operating characteristic curve (ROC AUC) and  accuracy as our main metrics for model performance. Accuracy is defined as the fraction of correctly predicted observations:

$$
\text{accuracy} = \frac{\text{TP + TN}}{\text{P+N}}
$$
where TN and TP are the number of true positives and negatives, respectively, and P and N are the number of real positive and regative cases in the data, respectively.

We notice, by looking at Figure \@ref(fig:hist1), that retained and non-retained small molecules are not equally present in our data. Due to this class imbalance accuracy alone is not a good metric, and hence we will consider other metrics as well. Sensitivity and specificity, which are also present in the ROC-curve, will be used and are defined as follows:

$$
\text{sensitivity} = \frac{\text{TP}}{\text{P}}
$$

and

$$
\text{specificity} = \frac{\text{TN}}{\text{N}}.
$$

We will also use the F1 score for assessing the final model. F1 score is defined as follows:

$$
\text{F1 score} = \frac{\text{2TP}}{\text{2TP + FP + FN}} \text{,}
$$
where FP and FN are the number of false positives and negatives, respectively.

### Metrics for regression

Regression models, used in predicting retention time, will be evaluated by using root mean squared error (RMSE), which is defined as follows:

$$
\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y_i})^2}
$$

where $y_i$ is the $i$th observation for the dependent variable, $\hat{y_i}$ is the corresponding prediction by the regression model, and $n$ is the amount of observations.

Furthermore, we will use coefficient of determination ($R^2$) to assess how much of the variation seen in the dependent variable can be explained with the predictors. The coefficient of determination is defined in the following way:

$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum_i (y_i - \hat{y_i})^2}{\sum_i (y_i - \bar{y})^2} \text{,}
$$

where RSS and TSS are the residual and total sum of squares, respectively, and $\bar{y}$ is the average value for the dependent variable.

The final regression model will also be evaluated by calculating the mean absolute error (MAE):

$$
\text{MAE} = \frac{\sum_{i=1}^n |\hat{y_i} - y_i|}{n}.
$$

MAE will help us compare model performance to the amount of experimental error observed in the retention time results.

\newpage


# Results & Discussion

Before diving into making predictions we will start this section by exploring the data. The results for our two main tasks are presented in separate sub-sections after exploratory data analysis.

## Exploratory data analysis

### Initial assessment of data

The dataset contains `r nrow(df)` observations with `r sum(is.na(df))` missing values. The `r ncol(df)` columns can be divided roughly into three categories: the dependent variable or the outcome (retention time), descriptive variables which can be used for identifying individual molecules, and the numerical features/predictors. The the first six values for the outcome variable along with the two ID variables are shown in Table \@ref(tab:variables1).

```{r}
## Predictors/features can be divided into subcategories
### below we define groups of different kinds of predictors

# Number of chemical elements in each molecule
elements <- c("C", "H", "N", "S", "Cl", "O", "F", "I", "Br", "Si", "P")

# Number of chemical groups and special atoms in each molecule
chem_groups <- c("RNH2", "R2NH", "R3N", "ROPO3", "ROH", "RCHO", "RCOR", "RCOOH", "RCOOR", "ROR", "RCCH", "RCN", "RINGS", "AROMATIC")

#Values for physico chemical properties
phys_chem <- c("MW", "Ncharges", "ALogP", "ALogp2", "AMR", "apol", "naAromAtom", "nAromBond", "TopoPSA", "fragC", "nHBAcc", "nHBDon", "nRotB", "VABC", "Zagreb", "ECCEN", "WPATH", "WPOL")
```


The predictors can be further divided into subclasses based on the type of information they present. Rough division of predictors is as follows:

* Number of chemical elements in a specific small molecule are listed in the following columns: `r elements`

* Number of chemical groups are listed in: `r chem_groups`

* Values for physicochemical properties are listed in: `r phys_chem`

The distribution of chemical elements are displayed in Figure \@ref(fig:elements) in the form of a boxplot. Carbon and hydrogen atoms are present in abundance, which is not a surprize since we are dealing with organic molecules. The violin plots in Figure \@ref(fig:elements) b) show the distributions of carbom and hydrogen in more detail.

```{r elements, fig.cap= "Boxplots of chemical element distributions (a) in the data are presented, along with violin plots (b) which show the distributions for carbon and hydrogen in greater detail.", fig.height=2.5}

# Boxplot of chemical element distributions
# Not sure if this is the best choise for discrete value variables, but can't think of a better choise
box1 <- df %>% dplyr::select(tidyselect::all_of(elements)) %>% 
  pivot_longer(cols = tidyselect::all_of(elements), names_to = "element_name", values_to = "atom_counts") %>% 
  ggplot(aes(x = element_name, y = atom_counts)) +
  geom_boxplot() +
  labs(x = "Chemical element", y = "Number of atoms in a molecule") +
  ggtitle("a)")

# Violin plot to display carbon and hydrogen
violin1 <- df %>% dplyr::select(C, H) %>% 
  pivot_longer(cols = c(C, H), names_to = "element_name", values_to = "atom_counts") %>% 
  ggplot(aes(x = element_name, y = atom_counts)) +
  geom_violin(fill = "gray") +
  labs(x = "Chemical element", y = "Number of atoms in a molecule") +
  ggtitle("b)")

grid.arrange(box1, 
             violin1, 
             ncol = 2,
             widths = c(3,1))


rm("box1", "violin1")

# df %>% select(elements) %>% 
#   pivot_longer(cols = all_of(elements), names_to = "element_name", values_to = "atom_counts") %>% 
#   ggplot(aes(x = atom_counts)) +
#   geom_histogram() +
#   facet_wrap(facets = "element_name")

```

```{r variables1}
#The outcome and ID variables
df %>% dplyr::select(rt, pubchem, MF) %>% 
  head() %>%
  rename("Retention time (s)" = rt, "PubChem ID" = pubchem, "Molecular Formula" = MF) %>% 
  knitr::kable(caption = "The outcome variable and the two ID variables") 

# Does not work for pdf output for some reason
#%>%
#  kable_styling(latex_options = "HOLD_position")
#  kableExtra::add_header_above(c("Outcome" = 1, "ID variable" = 2))
```


Chemical group counts are shown in Figure \@ref(fig:chemgroups). Many of the groups seem to be quite rare, whereas ring structures are commonly observed in the small molecules. The size of the rings in the dataset are limited to six carbon atoms or less, so if larger ring structures are present in a molecule they are not included in the count.

```{r chemgroups, fig.cap="The appearance of different chemical groups in the dataset molecules are represented as a dotplot. The size and color of the dot are indicative of the amount observations. RINGS shows the amount of ring structures (up to six carbon atoms in size), and AROMATIC indicates how many of those rings are aromatic in nature.", fig.height=3}
## Boxplot not very good since values are counts
# df %>% select(all_of(chem_groups)) %>% 
#   pivot_longer(cols = all_of(chem_groups), names_to = "group_name", values_to = "group_counts") %>% 
#   ggplot(aes(x = group_name, y = group_counts)) +
#   geom_boxplot() +
#   labs(x = "Chemical group", y = "Number of groups in a molecule") +
#   ggtitle("Distributions of chemical groups in the dataset") +
#   coord_flip()

#Dot plot, where the size is proportional to number of observations
df %>% dplyr::select(all_of(chem_groups)) %>% # get chemical groups
  pivot_longer(cols = tidyselect::all_of(chem_groups), names_to = "group_name", values_to = "group_counts") %>% 
  group_by(group_name, group_counts) %>% # Group by name of the group and amount in a molecule
  summarise(n = n()) %>%  # Summarize number of observations
  ggplot(aes(x = group_name, y = group_counts)) +
  geom_point(aes(size = n, color = n)) +
  labs(x = "Chemical group", y = "Number of groups in a molecule") +
  coord_flip()

```


Values for physicochemical properties are shown in Figure \@ref(fig:physchem). Since the value for the properties take differing units it is difficult to present them on the same scale. Therefore, the variables are divided into two categories in Figure \@ref(fig:physchem) based on the numeric property value to prevent the large-value variables from dwarfing the small-value ones. The variables with large numeric values represent the following molecular property descriptors:

* `WPATH`: the Wiener Path number [@Wiener1947]

* `fragC`: the calculated complexity of a system [@Nilakantan2006]

* `ECCEN`: the eccentric connectivity index [@Sharma1997].

The smaller numeric value variable contain:

* `MW`: the molecular weight

* `ALogP`: the logarithm of the partition coefficient according to Ghose and Crippen [@Ghose1986]

* `ALogp2`: the second power of `ALogP`

* `AMR`: the Ghose-Crippen molar refractivity [@Ghose1987]

* `apol`: the sum of the atomic polarizabilities

* `TopoPSA`:  the topological polar surface area based on fragment contributions (TPSA) [@Ertl2000]

* `VABC`: the volume of a molecule

* `Zagreb`: a descriptor for the sum of the squared atom degrees of all heavy atoms

* `WPOL`: the Wiener Polarity number [@Wiener1947].


```{r physchem, fig.cap="The distributions of physicochemical descriptors are shown in the form of boxplots. Variables with larger numeric values are shown on the left-hand side.", fig.height=3}
#Boxplot of phys chem properties

df %>% dplyr::select(tidyselect::all_of(phys_chem)) %>% 
  pivot_longer(cols = tidyselect::all_of(phys_chem), names_to = "prop_name", values_to = "prop_value") %>% 
  # Lets take properties with large values and put them to a separate facet
  mutate(value_type = if_else(prop_name %in% c("WPATH", "fragC", "ECCEN"), "Large values", "Small values")) %>% 
  ggplot(aes(x = prop_name, y = prop_value)) +
  geom_boxplot() + coord_flip() +
  # Free scales to allow for different values of property value scale
  facet_grid(. ~value_type, scales = "free") +
  labs(y= "Property value", x = "")

# Count values
pc_count_vars <- c("nRotB", "nHBDon", "nHBAcc", "Ncharges", "nAromBond", "naAromAtom")
```

Additionally, some of the physicochemical descriptors, namely `r pc_count_vars`, are not continuous but rather counts, and are thus poorly displayed as a boxplot. These variables convey the following information:

* `nRotB`: the number of non-rotatable bonds on a molecule 

* `nHBDon`: the number of hydrogen bond donors

* `nHBAcc`: the number of hydrogen bond acceptors

* `Ncharges`: the number of charged atoms

* `nAromBond`: the number of aromatic bonds

* `naAromAtom`: the number of aromatic atoms  

Figure \@ref(fig:physchemcount) shows the above mentioned variables in the form of histograms. Ncharges variable seems to have mainly zero-count observations, whereas the other variables in the figure are distributed more evenly.

```{r physchemcount, fig.cap = "Values of certain physicochemical descriptors displayed as histograms. The largest observed value for each descriptor is depicted in the respective pane. Note the logarithmic y-axis.", fig.height=3}

df %>% dplyr::select(tidyselect::all_of(pc_count_vars)) %>% 
  pivot_longer(cols = tidyselect::all_of(pc_count_vars), 
               names_to = "prop_name", 
               values_to = "prop_value") %>%
  ggplot(aes(x = prop_value)) +
  geom_histogram(fill = "red", color = "black", binwidth = 1) +
  # Facet each count variable into individual pane
  facet_wrap(facets = "prop_name") +
  scale_y_log10() +
  labs(x = "Property value", y = "Number of observations") +
  # Print the largest observation value as text on each facet
  geom_text(data = df %>% dplyr::select(tidyselect::all_of(pc_count_vars)) %>% 
              pivot_longer(cols = tidyselect::all_of(pc_count_vars), 
                           names_to = "prop_name", 
                           values_to = "prop_value") %>%
              group_by(prop_name) %>% 
              summarise(range_high = range(prop_value)[2]), #high end of range
            aes(x = 20, y = 60000, label = paste("Largest value:", range_high))
              )

```



### Correlation between numerical data

All of the predictors are numerical variables, so let's examine their correlation. A visualization of the correlation matrix with the predictors and the outcome is shown in Figure \@ref(fig:correlation). The variables at the bottom of the matrix seem to be highly correlated with each other.

```{r correlation, fig.cap= "Visual representation of the upper half of the correlation matrix of retention time and the numerical predictors. The column names have been changed to column numbers to clarify the visualization. The column names and numbers are shown together as row names in the correlation matrix."}
#Create a pretty correlogram here
#Build correlogram
M <- df %>% dplyr::select(-MF, -pubchem) %>% cor()

# rename rows and columns for better readability
original_rownames <- row.names(M)
#Let's change column names into numbers to make the graph less cluttered
colnames(M) <- c("rt", 2:dim(M)[2])
# Add numbers to row names as well
row.names(M) <- paste0(row.names(M), paste(c("", rep(",", times = 43)), c("", 2:44)))

corrplot(M, method = 'ellipse', tl.cex = 0.7, type = "upper")
```


Retention time has the highest correlation with `r original_rownames[which.max(M[1,2:44])+1]` with a value of `r round(max(M[1,2:44]), digits = 2)`. The highest negative correlation is with the variable `r original_rownames[which.min(M[1,2:44])+1]` with a value of `r round(min(M[1,2:44]), digits = 2)`. For other variables, the absolute values of correlation with retention time remain below 0.5. Now that we have a general understanding on the structure of the data, let's briefly look at a few visualizations regarding the aims of this study.

### Non-retained molecules

Earlier we defined molecules as non-retained, if they travel through the LC column is less than 300 seconds. This choice is somewhat arbitrary, and by no means a precise definition. However, by looking at Figure \@ref(fig:hist1), we notice a clear gap between the retained and non-retained molecules, which supports our choice for the cut-off value between the classes.

```{r}
#How many small molecules are not retained? 

#df %>% mutate(not_retained = (rt<300)) %>% group_by(not_retained) %>% summarise(n = n())

```


```{r boxplot1, fig.cap= "Boxplot of a) the partition coefficient and b) the molecular weight of retained and non-retained molecules.", fig.height=3}
### Visualizing not_retained molecules -----------

#Box-plot logP by class
box_logp <- df %>% 
  mutate(not_retained = (rt<300)) %>%
  ggplot(aes(y = ALogP, x = not_retained)) +
  geom_boxplot() +
  ggtitle("a)") +
  labs(x = "Molecule not retained")

#Box-plot MW by class
box_MW <- df %>% 
  mutate(not_retained = (rt<300)) %>%
  ggplot(aes(y = MW, x = not_retained)) +
  geom_boxplot() +
  ggtitle("b)") +
  labs(x = "Molecule not retained", y = "Molecular weight")

#Plots side-by-side
grid.arrange(box_logp, 
             box_MW, 
             ncol = 2
             )

#remove plot objects
rm("box_logp", "box_MW")

```

Based on Figure \@ref(fig:correlation), retention time had the highest correlation with the logarithm of the partition coefficient, `ALogP`. Moreover, we could postulate that smaller molecules travel through the LC column with relative ease. Figure \@ref(fig:boxplot1) visualizes `ALogP` and molecular weight of retained and non-retained molecules. We notice that for both variables, the distributions of the two molecule classes are slightly shifted. This suggest that these variables might be useful in building a classification model. We will explore this topic further as we start building classification models.


Let's also look at principal component analysis (PCA) with the two classes of molecules. Figure \@ref(fig:PCA) a) shows the biplot of the first two principal components, which reveals that even though the retained and non-retained molecules are somewhat separated, they are still mostly mixed. Furthermore, looking at Figure \@ref(fig:PCA) b), we notice that the first two principal components only explain less than 40% of the variance. It would seem that using PCA will not provide any clear benefit for classification.

```{r PCA, fig.cap= "a) Biplot showing the first two principal components. The retained and non-retained molecules are coloured differently. b) Cumulative proportion of variance explained by the principal components.", fig.height=2.8}
# # Add classification variable
# df2 <- df %>%
#   mutate(not_retained = rt<300) %>%
#   dplyr::select(-rt, -pubchem, - MF) # Remove outcome and ID variables
# 
# #Fit PCA for predictors
# pca_fit <- prcomp(x = dplyr::select(df2, -not_retained), scale = TRUE)
# 
# #plot(pca_fit)
# 
# # Biplot with the two classes
# biplot_1 <-
#   ggplot(data = data.frame(PC1 = pca_fit$x[,1], PC2 = pca_fit$x[,2], not_retained = df2$not_retained),
#        aes(x = PC1, y = PC2, color = not_retained)) +
#   geom_point(alpha = 0.5) +
#   labs(x = "Principal component 1", y = "Principal component 2") +
#   theme(legend.position = "bottom") +
#   ggtitle("a)")
# 
# #Proportion of variance explained
# pr.var <- pca_fit$sdev^2
# #pr.var
# #proportion of variance explained
# pve <- pr.var / sum(pr.var)
# 
# #Cumulative proportion of variance explained
# var_plot <-
#   ggplot(data = data.frame(cum_var = cumsum(pve), PC = 1:length(pve)),
#        aes(x = PC, y = cum_var)) +
#   geom_line() +
#   geom_point(size = 2, color = "red") +
#   labs(x = "Principal component", y = "Variance explained") +
#   ggtitle("b)")
# 
# #Plots side-by-side
# pca_plots <- grid.arrange(biplot_1,
#              var_plot,
#              ncol = 2
#              )
# 
# ggsave("pca_plots.png", plot = pca_plots, dpi = 150, device = "png", width = 7, height = 3, units = "in")
# 
# rm("df2", "pca_fit", "biplot_1", "var_plot", "pve", "pr.var", "pca_plots")

## Please uncomment code above if you want to run PCA code
#The result output has been saved to a PNG-file to reduce pdf-report file size and knit-time

#include pca output from intermediate png-file
knitr::include_graphics(path = "pca_plots.png")

```


### Retention time

As we discussed earlier, `ALogP` has the largest correlation with retention time. Figure \@ref(fig:scatter1) visualizes relationship between `ALogP` and retention time. Non-retained molecules seem to lie as an uniform group at the bottom (i.e. near-zero correlation), whereas there seems to be a tendency for increasing retention time values as the `ALogP` values increase for retained molecules.

```{r scatter1, fig.cap= "Retention time in seconds as a function of the logarithm of partition coefficient.", fig.height=2, fig.width= 4}
### Visualizing not_retained molecules -----------
# 
# scatter1 <- df %>% mutate(not_retained = (rt<300)) %>%
#   ggplot(aes(y = rt, x = ALogP)) +
#   geom_point(aes(color = not_retained)) +
#   labs(y = "Retention time (s)")
# 
# ggsave("scatterplot.png", plot = scatter1, dpi = 150, device = "png", width = 4, height = 2.5, units = "in")
# 
# rm("scatter1")

## Please uncomment code above if you want to run it
# The plot has been saved as a png-file to reduce pdf-report file size

knitr::include_graphics(path = "scatterplot.png")

```

\newpage

## Classification of non-retained and retained molecules

In this section we will try different classification methods for evaluating whether a molecule is retained in the LC column or not.

Let's start by splitting the data to test and train sets. we will use 75% of the data for model building and comparison purposes (training set) and 25% for final model performance evaluation (test set). The amount of observations after the split are shown in Table \@ref(tab:classsplit). We can see that the data is quite imbalanced and retained molecules outnumber the non-retained ones. Nevertheless, the proportion of observations between the test and train sets are comparable, which is one of the things we paid attention to whilst making the data split. Looking at Table \@ref(tab:classsplit), our baseline accuracy, by assuming a model which categorizes every molecule as retained, is around 97.4 %. This is a tough benchmark to beat, but let's see how well our models fare.

We will test the following models for classification:

* Logistic regression

* Linear discriminant analysis (LDA)

* Quadratic discriminant analysis (QDA)

* Random Forest.

The final model will be chosen by comparing model performance. We will use 10-fold cross-validation on the train set to assess the performance of several models. Due to class imbalance, other measures besides accuracy, such as specificity and area under the receiver operating characteristic curve (ROC AUC) will also be used. The final classification model performance will be evaluated against the test set using accuracy, sensitivity, specificity, and F1 score as evaluation metrics.


```{r classsplit}
### Test/Train split ----------------------

set.seed(1234)
#split to test train 25:75
classification_split <- initial_split(data = df %>% mutate(not_retained = (rt<300) %>% as.factor()), 
                                      prop = 3/4, 
                                      strata = not_retained)


#Create test and train
df_test <- testing(classification_split)
df_train <- training(classification_split)

### Create 10-fold cross-validation dataset for train -----------

train_folds <- vfold_cv(df_train, v = 10, strata = not_retained)


# How are the retained vs non-retained molecules divided?
test_train_class <- df_test %>%  
  count(not_retained) %>% 
  mutate(prop = n/sum(n), set = "test") %>% 
  bind_rows(
    df_train %>%  
      count(not_retained) %>% 
      mutate(prop = n/sum(n), set = "train")
  )

# Print amount of molecules as table
test_train_class %>% 
  knitr::kable(caption = "The amount and proportion of observations in different classes for the test and train sets")

rm("test_train_class")
```

### Logistic regression

In logistic regression the probability of a molecule not being retained in the LC column ($p_{\text{nr}}$) is modelled using the logistic function:

$$
p_{\text{nr}} (X) = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_pX_p}},
$$

where $\beta_0$ and $\beta_1 ... \beta_p$ are the regression coefficients and $X = (X_1, ..., X_p)$ are $p$ predictors [@ISLRV2]. Fitting is used for determining values for individual regression coefficients, $\beta_i$.


We start by setting the values in `rt`, `pubchem` and `MF` columns as ID variables, which will not be used in fitting the model. The true classes are defined by a new column named `not_retained`, which is defined as being `TRUE` for molecules with `rt < 300`. Logistic regression models will be fitted with the remaining predictors.


```{r fullmodel, cache=TRUE}
# all predictors


# recipe for classification drop retention time as "ID variable"
class_recipe_all <-
  recipe(not_retained ~ ., data = df_train) %>% #retention is the outcome we want to predict
  update_role(rt, pubchem, MF, new_role = "ID") %>%  # rt, pubchem id, and molecular formula as ID variables (not predictors)
  step_zv(all_predictors()) # remove zero variance variables (if any)

## Logistic regression model specification
lr_model_spec <-
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Workflow

class_wflow <-
  workflow() %>%
  add_model(lr_model_spec) %>%
  add_recipe(class_recipe_all)

## The 10-fold crossvalidation results have been saved to a csv-file and the code has been commented to save time while knitting
# Please uncomment the lines below if you wish to run the code

# 
# #fit logistic regression using tidymodels all predictors
# class_lr_fit1 <- 
#   class_wflow %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Let's collect metrics from our 10-fold cross-validation
# # collect_metrics(class_lr_fit1) %>% 
# #   knitr::kable(caption = "Metrics for Logistic regression model with all predictors")
# 
# 
# ## compare models in tabular format
# model_comp <- 
#   collect_metrics(class_lr_fit1) %>% mutate(model = "Logistic", complexity = "all vars")


#Function for adding new model metrics to comparison table
add_model_comp <- function(models = model_comp, new_fit, model, complexity){
  new_comparison <- models %>% 
    bind_rows(collect_metrics(new_fit) %>% 
              mutate(model = model, complexity = complexity)
              )
  return(new_comparison)
}

```

First we fit a model with all variables as predictors. This leads to a pretty complicated model, so let's see if we can simplify our model by reducing the amount of predictors. The simplest model might contain only `ALogP` and `Mw` as predictors, since we saw in Figure \@ref(fig:boxplot1) that these variables seem to be related to the retainability of a molecule.

```{r alllogmodels, cache=TRUE}

# Simple model with just two predictors
# New simple recipe:

class_recipe_simple <-
  recipe(not_retained ~ MW + ALogP, data = df_train)

class_wflow2 <-
  workflow() %>%
  add_model(lr_model_spec) %>%
  add_recipe(class_recipe_simple)

## The 10-fold crossvalidation results have been saved to a csv-file and the code has been commented to save time while knitting
# Please uncomment the lines below if you wish to run the code

# 
# #10-fold cv with simple LogReg-model
# class_lr_fit2 <- 
#   class_wflow2 %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Let's collect metrics from our 10-fold cross-validation
# model_comp <- add_model_comp(models = model_comp,
#                              new_fit =class_lr_fit2,
#                              model = "Logistic", 
#                              complexity = "simple"
#                             )
# 
# # Halfway model (takeaway the atom count variables)
# 
# Number of chemical elements in each molecule
elements <- c("C", "H", "N", "S", "Cl", "O", "F", "I", "Br", "Si", "P")
# Variables which are highly correlated with other variables
high_corr <- c("ALogp2", "apol", "ECCEN", "WPOL")

#New model recipe
class_recipe_intermed <-
  recipe(not_retained ~ ., data = df_train) %>% #retention is the outcome we want to predict
  update_role(rt, pubchem, MF, new_role = "ID") %>%  # rt, pubchem id, and molecular formula as ID variables (not predictors)
  update_role(all_of(elements), new_role = "ID") %>% #Make chemical elements just ID variables
  update_role(all_of(high_corr), new_role = "ID") %>% # Make some highly correlated variables ID variables
  step_zv(all_predictors()) # remove zero variance variables (if any)

class_wflow3 <-
  workflow() %>%
  add_model(lr_model_spec) %>%
  add_recipe(class_recipe_intermed)


## The 10-fold crossvalidation results have been saved to a csv-file and the code has been commented to save time while knitting
# Please uncomment the lines below if you wish to run the code

#   
# #10-fold cv with simple LogReg-model
# class_lr_fit3 <- 
#   class_wflow3 %>% 
#   fit_resamples(resamples = train_folds)
#   
# # Let's collect metrics from our 10-fold cross-validation
# model_comp <- add_model_comp(models = model_comp,
#                              new_fit =class_lr_fit3,
#                              model = "Logistic", 
#                              complexity = "intermediate"
#                             )
# 
# #Let's write the metrics to a csv-file, so that we save time when knitting several times
# write.csv(model_comp, file = "classificationmetrics1.csv", row.names = FALSE)

## Please uncomment the lines above if you wish to run the code

# Read cross-validation results from a file
model_comp <- read.csv("classificationmetrics1.csv") %>% as.tibble()

model_comp %>% 
  dplyr::select(-.config) %>% 
  knitr::kable(caption = "Metrics for different logistic regression models were calculated based on 10-fold cross-validation", digits = 3)
```

An intermediate option would be to drop variables which we suspect not being related to the prediction outcome. Let's drop all chemical elements from our intermediate logistic regression model, namely the following columns: `r elements`. Furthermore, we will drop a few variables which are highly correlated with some of the other variables, namely `r high_corr`.

Estimated values for accuracy and ROC AUC based on 10-fold cross-validation for logistic regression models with different amount of variables are shown in Table \@ref(tab:alllogmodels). Accuracy is pretty similar for all models regardless of the amount of variables. However, ROC AUC is considerably improved with increasing model complexity. The intermediate model, which contains `r length(c(elements, high_corr))` variables less than the most complex model, performs nearly as well as the model which uses all predictors.

### LDA, QDA & random forest

Logistic regression already gave results which were better than the baseline prediction accuracy of assuming all molecules as being retained. We can also try a few other supervised learning techniques for the classification task, and see if we can improve still. We will apply Linear and Quadratic Discriminant Analysis (LDA & QDA), and random forest to the classification task.

Both LDA and QDA are based on the Bayes' theorem, where the probability that observation $Y$ belongs to a class $k$, given a certain predictor value $X=x$ is expressed by the following formula:

$$
\Pr(Y = k|X=x) = \frac{\pi_kf_k(x)}{\sum_{l=1}^{K} \pi_lf_l(x)} \text{,}
$$
where $K$ is the number of classes (in our case $K=2$), $\pi_k$ is the overall prior probability that a random observation belongs to the $k$th class, and $f_k (X)$ is the density function of $X$ for an observation belonging to the $k$th class [@ISLRV2]. Both LDA and QDA assume that the observations for each class are drawn from a multivariate normal distribution. The difference between LDA and QDA comes from the fact that LDA assumes that the $K$ classes share a common covariance matrix, where as QDA estimates a separate covariance matrix for each class [@ISLRV2]. The main practical difference resulting from this is that QDA is more flexible but also computationally more heavy. With $p$ predictors, estimation of a covariance matrix requires estimating $p(p+1)/2$ parameters [@ISLRV2]. For this reason, we will only test QDA with the so-called simple setting where the only  predictors are `ALogP` and `MW`, and the intermediate setting, where the amount of predictors has been reduced by dropping the chemical elements and a few highly correlated variables. We used these same predictor selections earlier with logistic regression.

In a random forest model, we build several decision trees on bootstrapped training samples to estimate the class of an observation. Additionally, at each split we only use $m$ out of the $p$ predictors ($m<p$). This can be especially helpful if we have a large number of correlated predictors [@ISLRV2]. Classification result for an individual observation will be the most common class predicted by the trees in the model.


```{r}
## The 10-fold crossvalidation results have been saved to a csv-file and the code has been commented to save time while knitting
## Please uncomment the lines below if you wish to run the code

# set.seed(111)
# 
# ### Linear discriminant analysis ----------------------------
# 
# ## model specification
# 
# lda_model_spec <- discrim_linear() %>% 
#   set_mode("classification") %>% 
#   set_engine("MASS")
# 
# # Workflow for simple LDA model
# lda_wf_1 <-
#   workflow() %>% 
#   add_model(lda_model_spec) %>% 
#   add_recipe(class_recipe_simple)
# 
# # Fit simple model:
# #10-fold cv with simple LDA-model
# class_lda_fit1 <- 
#   lda_wf_1 %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = class_lda_fit1, model = "LDA", complexity = "simple")
# 
# # Workflow for intermediate LDA model
# lda_wf_2 <-
#   workflow() %>% 
#   add_model(lda_model_spec) %>% 
#   add_recipe(class_recipe_intermed)
# 
# # Fit intermediate model:
# #10-fold cv with LDA-model
# class_lda_fit2 <- 
#   lda_wf_2 %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = class_lda_fit2, model = "LDA", complexity = "intermediate")
# 
# 
# # Workflow for LDA model with all predictors
# lda_wf_3 <-
#   workflow() %>% 
#   add_model(lda_model_spec) %>% 
#   add_recipe(class_recipe_all)
# 
# # Fit full model:
# #10-fold cv with full LDA-model
# class_lda_fit3 <- 
#   lda_wf_3 %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = class_lda_fit3, model = "LDA", complexity = "all vars")
# 
# ## Quadratic discriminant analysis -------------------
# 
# ## model specification
# qda_model_spec <- discrim_quad() %>% 
#   set_mode("classification") %>% 
#   set_engine("MASS")
# 
# # Workflow for simple QDA model
# qda_wf_1 <-
#   workflow() %>% 
#   add_model(qda_model_spec) %>% 
#   add_recipe(class_recipe_simple)
# 
# # Fit simple model:
# #10-fold cv with simple QDA-model
# class_qda_fit1 <- 
#   qda_wf_1 %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = class_qda_fit1, model = "QDA", complexity = "simple")
# 
# # Workflow for intermediate QDA model
# qda_wf_2 <-
#   workflow() %>% 
#   add_model(qda_model_spec) %>% 
#   add_recipe(class_recipe_intermed)
# 
# # Fit intermediate model:
# #10-fold cv with QDA-model
# class_qda_fit2 <- 
#   qda_wf_2 %>% 
#   fit_resamples(resamples = train_folds) #Fails for two folds due to rank deficiency
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = class_qda_fit2, model = "QDA", complexity = "intermediate")
# 
# ## Random forest -----------------
# 
## Model specification
rf_model_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("classification")

## Workflow for random forest with all variables

rf_wf <- workflow() %>%
  add_model(rf_model_spec) %>%
  add_recipe(class_recipe_all)

## The 10-fold crossvalidation results have been saved to a csv-file and the code has been commented to save time while knitting
# Please uncomment the lines below if you wish to run the code

# # #Simple model test
# # rf_fit1 <- rf_wf %>% fit(df_train)
# # 
# # #Predictions 
# # rf_preds <- rf_fit1$fit$fit$fit$predictions %>% as.data.frame()
# # names(rf_preds) <- c("p_retained", "p_not_retained")
# # 
# # rf_preds <- rf_preds %>% mutate(pred = if_else(p_not_retained >= 0.5, TRUE, FALSE) %>% as.factor())
# # 
# # caret::confusionMatrix(rf_preds$pred, df_train$not_retained)
# 
# #10-fold cross-validation
# rf_fit <- 
#   rf_wf %>% 
#   fit_resamples(resamples = train_folds)
# 
# # Add metrics to model comparison
# model_comp <- add_model_comp(new_fit = rf_fit, model = "Random Forest", complexity = "all vars")
# 
# #Write metrics to a csv-file
# write.csv(model_comp, file = "classificationmetrics2.csv", row.names = FALSE)


#Read metrics from file
model_comp <- read.csv("classificationmetrics2.csv") %>% as.tibble()

```


After applying these new methods to our classification problem, random forest and logistic regression show the highest accuracy. Comparison of model performance in terms of accuracy is shown in Figure \@ref(fig:accuraryInclassification). Several models perform above the baseline accuracy, which comes from assuming all molecules as being retained in the LC column.

```{r accuraryInclassification, fig.cap="The mean accuracy of different models is shown in the form of a dotplot. The length of the error bars correspond with standard error defined through 10-fold cross-validation. The red dashed line indicates the baseline prediction accuracy which we aim to beat.", fig.height=3}
# Baseline prediction performance for accuracy = all molecules not retained
bl_pred <- sum(df$rt>300)/length(df$rt)
## Compare models

#accuracy
model_comp %>% mutate(model_t = paste(model, complexity, sep = " ")) %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(x = model_t, y = mean, color = (mean > bl_pred))) +
  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) +
  geom_hline(yintercept = bl_pred, color = "red", lty = 2) +
  labs(y = "Accuracy", x = "Model & Complexity") +
  scale_color_discrete(name="Accuracy above baseline:") +
  theme(legend.position = "top") +
  coord_flip()

```

QDA does not seem to do well compared to the other models, whereas the performance of the simple version of LDA with just two predictors is quite comparable to the simple logistic regression.

ROC AUC was also assessed to get a better idea of model performance. Figure \@ref(fig:rocauc1) shows the average ROC AUC values for the different models. Again, random forest and logistic regression with all the predictors perform the best.

```{r rocauc1, fig.cap="The mean ROC AUC values for different models is shown in the form of a dotplot. The length of the error bars correspond with standard error defined through 10-fold cross-validation.", fig.height=3}
#ROC auc
model_comp %>% mutate(model_t = paste(model, complexity, sep = " ")) %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = model_t, y = mean)) +
  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) +
  labs(y = "ROC AUC", x = "Model & Complexity") +
  coord_flip()

```


```{r bestclassmodels}
#Tabular comparison for two best models
model_comp %>% 
  dplyr::filter(model %in% c("Logistic", "Random Forest")) %>% 
  dplyr::filter(complexity == "all vars") %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(
    caption = "Accuracy and ROC AUC for the two best models",
    digits = 4
  )
```

The metrics for the two best performing models are presented in Table \@ref(tab:bestclassmodels). We notice that performance is very similar for both models in terms of the metrics used here. Let's compare the performance of these two algorithms in more detail.

### Choosing the final model

We will compare the performance of random forest and logistic regression using all predictors for training the models. 2-fold cross-validation will be used. That is to say, we split the train set into two pieces and assess model performace with these data sets. Training and validation of performance will be done twice, once per each half.

```{r}
### Compare random forest and logistic regression ------------
# Let's compare the two best models by splitting the training data into two parts

#split train data 50:50
classification_split <- initial_split(data = df_train %>% 
                                        mutate(not_retained = (rt<300) %>% 
                                                 as.factor()), 
                                      prop = 1/2, 
                                      strata = not_retained)


#Create two training folds
train1 <- testing(classification_split)
train2 <- training(classification_split)


# Metrics which we will check
multi_metric <- metric_set(accuracy, yardstick::sensitivity, yardstick::specificity, f_meas)
```


```{r fold1}
## Train models with first half of training data, i.e. train1
#fit logistic regression using tidymodels all predictors
LR_fit_train1 <- 
  class_wflow %>% 
  fit(data = train1)

#Random forest on first fold of train data
RF_fit_train1 <- 
  rf_wf %>% 
  fit(data = train1)

##compare models
models <- list("Logistic regression, fold 1" = LR_fit_train1,
               "Random forest, fold 1" = RF_fit_train1)

preds1 <- imap_dfr(models, augment, 
                  new_data = train2, .id = "model")

```


```{r fold2}
## Train models with second half of training data, i.e. train2

#Logistic regression
LR_fit_train2 <- 
  class_wflow %>% 
  fit(data = train2)

#Random forest fold2
RF_fit_train2 <- 
  rf_wf %>% 
  fit(data = train2)

##compare models
models <- list("Logistic regression, fold 2" = LR_fit_train2,
               "Random forest, fold 2" = RF_fit_train2)

preds2 <- imap_dfr(models, augment, 
                  new_data = train1, .id = "model")

```


Figure \@ref(fig:Finalcomparison) shows the ROC-curve comparison for both folds. The ROC-curves for both models are very similar, and both models seems to perform equally well. We could of course further fine tune model parameters in order to optimize model performance. For example, with random forest we could try to adjust the number of trees and the amount of predictors $m$ used during each tree split. However, as our main objective lies in predicting retention time, we will not invest more time into parameter tuning at this point. Instead, we will choose the logistic regression as our final classification model, given the smaller computational resources required by this model. Next, we will assess the performance of logistic regression on the test set.

```{r Finalcomparison, fig.cap="ROC curves for logistic regression and random forest models trained on the first and second fold of training data. The perfomance of the models is very similar.", fig.height= 3}
## Comparison of both folds ----

fold1 <- preds1 %>% group_by(model) %>% roc_curve(truth = not_retained, estimate = .pred_FALSE)
fold2 <- preds2 %>% group_by(model) %>% roc_curve(truth = not_retained, estimate = .pred_FALSE)

bind_rows(fold1, fold2) %>% 
  separate(col = model, into = c("model", "fold"), sep = ", ") %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, color = model, lty = model)) +
  geom_line(size = 1.2, alpha = 0.9) +
  geom_abline(lty = 3) +
  facet_grid(. ~ fold) +
  theme(legend.position = "top")

```

### Final classification model

Here we will fit our final classification model using the full training data set. Moreover, we will evaluate model performance with test data.

The terms with a $p$-value below 0.05 and estimated absolute regression coefficient value above one for the final model trained on the full training data set are shown in Table \@ref(tab:finalregvalues). `ALogP` and `apol` have the highest negative regression coefficient values, whereas the number of carboxylic acid groups `RCOOH` and the amount of esters `RCOOR` have the highest positive values.

```{r finalregvalues}
## Training the final model with logistic regression -----
final_fit <- 
  class_wflow %>% 
  fit(data = df_train)

#Regression coefficients with p < 0.05
final_fit %>% 
  tidy %>% 
  filter(p.value<0.05) %>% 
  filter(abs(estimate) > 1) %>% 
  knitr::kable(caption = "Regression coefficients for terms with a p-value below 0.05 and absolute estimated value above one for the final logistic regression model")

```

Figure \@ref(fig:confMatrix) shows the confusion matrix of final model prediction performance on test data as a heatmap. Retained molecules are properly classified is most cases. However, only `r round(100*132/(132+387), digits =1)` % of the non-retained molecules are correctly classified. Classification metrics for final model performance on test data are shown in Table \@ref(tab:classificationMetrics). The model outperforms the baseline performance of the naive classifier model which would label all molecules as retained. However, the specificity of the final model is not very good.

```{r confMatrix, fig.cap="Confusion matrix showing the predicted and true classes for the dependent variable `not_retained`.", fig.height= 3, fig.width=3.5}

#Confusion matrix
augment(final_fit, new_data = df_test) %>%
  conf_mat(truth = not_retained, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```


```{r classificationMetrics}

#metrics for final classification model
augment(final_fit, new_data = df_test) %>%
  multi_metric(truth = not_retained, estimate = .pred_class) %>% 
  knitr::kable(caption = "Accuracy, sensitivity, specificity, and F1 score as metrics for the final classification model performance")

```

\newpage

## Predicting retention time

In this section we will build models for predicting retention time. However, before making predictions, we should have an idea on the magnitude of experimental error included in the measured retention time values. All laboratory methods contain some level of intrinsic experimental error, and liquid chromatography is no exception. For example the LC column, which needs to be replaced regularly, has an effect on the measured values. When collecting the experimental results, the authors of the original study used a subset of 198 molecules to determine the variability of measurement results. Each of the 198 molecules was analyzed at least twice with a difference of at least 30 days. Variability was calculated for each individual molecule, and the observed mean and median variability for retention time was 36 and 18 s, respectively [@Xavier2019a]. This gives us some perspective when assessing the performance of regression models. For example, mean absolute error below 36 s on training data is indicative of overfitting.

```{r}
## Let's use the mean value  of retention time as the "null model"
# This is the baseline for prediction accuracy we should be able to beat

# Mean rt for retained molecules
rt_mean <- df %>% dplyr::filter(rt>=300) %>% pull(rt) %>% mean()

# Function for calculating RMSE
calculate_rmse <- function(prediction, truth){
  RMSE <- sqrt(mean((truth - prediction)^2))
  return(RMSE)
}

#RMSE baseline from the null model
baseline_prediction_error <- calculate_rmse(prediction = rt_mean, 
                                      truth = filter(df, rt >=300) %>% pull(rt))
```


On the opposite side of the error spectrum lies the RMSE we get by predicting the mean retention time of retained molecules as `rt` for all molecules. The RMSE value for this so-called null model is `r round(baseline_prediction_error, digits = 1)` s. This is our baseline prediction error, which we should be able to improve upon.

In order to develop and test different models, we again split our data in 25:75 proportions into test and training sets, respectively. The non-retained molecules with `rt` values below 300 s have been filtered out. Cross-validation will be used for assesing and comparing the performance of different models before final model validation on test data. Figure \@ref(fig:retentiondistributions) shows that the distributions for retention time values are similar for both train and test sets.

```{r}
# Creating the data for regression modelling
### Test/Train split ----------------------

## filter out non-retained molecules
df <- df %>% dplyr::filter(rt>=300)

set.seed(1234)
#split to test train 25:75
regression_split <- initial_split(data = df, 
                                  prop = 3/4#, strata = rt
                                  )

#Create test and train
df_test <- testing(regression_split)
df_train <- training(regression_split)


### Create 10-fold cross-validation dataset for train -----------

train_folds <- vfold_cv(df_train, v = 10)

```


```{r retentiondistributions, fig.cap="Distribution of retention time for train (a) and test (b) sets.", fig.height= 3}
# is retention time similarly distributed in test and train?

hist1 <- df_train %>% 
  ggplot(aes(x = rt)) + 
  geom_histogram(fill = "red", color = "black") + 
  labs(x = "Retention time (s)") + 
  ggtitle("a) train set")

hist2 <- df_test %>% 
  ggplot(aes(x = rt)) + 
  geom_histogram(fill = "red", color = "black") + 
  labs(x = "Retention time (s)") + 
  ggtitle("b) test set")

grid.arrange(hist1, 
             hist2, 
             ncol = 2
             )

rm(hist1, hist2)
```


### Linear regression

As a benchmark we will train a few linear regression models and see how well they do in predicting retention time.
Let's begin with a simple model, where retention time is the expressed by the following formula:

```{r}

#Simple model
lm_fit <- lm(rt ~ MW + ALogP, data = df_train)
# summary(lm_fit)
equatiomatic::extract_eq(lm_fit)


```

The simple model shown above was evaluated using 10-fold cross-validation on the training data. The RMSE and $R^2$ values are shown in Table \@ref(tab:simplelm). The RMSE values are well above the mean experimental error of 36 s, but at the same time below the baseline RMSE of the null model (`r round(baseline_prediction_error, digits = 1)` s).

```{r simplelm}
## Tidymodels fit with 10-fold cross-validation

## Linear regression model specification
lm_spec <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")


### Simple lm model ----
recipe_simple <- 
  recipe(rt ~ MW + ALogP, data = df_train) #retention time is the outcome we want to predict

# Workflow

lm_wflow <- 
  workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(recipe_simple)


#fit linear regression using tidymodels
lm_fit1 <- 
  lm_wflow %>% #fit(df_train)
    fit_resamples(resamples = train_folds)

# Let's collect metrics from our 10-fold cross-validation
collect_metrics(lm_fit1) %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(caption = "RMSE and R-squared metrics for the simple linear regression model",
               digits = 4)

```

```{r}
## Which variables have near zero variance?
low_variance <- caret::nearZeroVar(df_train)
```


In order to improve model performance, we can try adding more predictors. The following variables `r names(df_train)[low_variance]` display low variance, and hence, might not add much information when trying to predict retention time. Let's fit two new models, one with all numerical features as predictors, and other with the low variance variables removed. Table \@ref(tab:newmodels) shows the metrics for these two models. We notice, that indeed the difference between the two models is not that large. Moreover, both models improve in terms of the metrics compared to the simpler linear model. Table \@ref(tab:variablesinlinear) shows some of the statistically most significant terms in the linear model with all predictors fitted on the entire training set. The most significant predictors, in terms of p-value, are `ALogP`, `nHBDon`, and `R3N`.


```{r newmodels}
## Model with low variance variables removed
recipe_intermed <- 
  recipe(rt ~ ., data = df_train) %>%  #retention time is the outcome we want to predict
  update_role(pubchem, MF, new_role = "ID") %>%   #pubchem id, and molecular formula as ID variables (not predictors)
  update_role(all_of(low_variance), new_role = "ID")

# Workflow
lm_wflow <- 
  workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(recipe_intermed)

#fit linear regression using tidymodels
lm_fit2 <- 
  lm_wflow %>% #fit(df_train)
  fit_resamples(resamples = train_folds)

# all predictors
recipe_all <- 
  recipe(rt ~ ., data = df_train) %>% #retention time is the outcome we want to predict
  update_role(pubchem, MF, new_role = "ID")

# Workflow

lm_wflow <- 
  workflow() %>% 
  add_model(lm_spec) %>% 
  add_recipe(recipe_all)

#fit linear regression using tidymodels all predictors
lm_fit3 <-
  lm_wflow %>%
  fit_resamples(resamples = train_folds)


# Let's collect metrics from our 10-fold cross-validation
collect_metrics(lm_fit2) %>% 
  mutate(model = "intermediate") %>% 
  bind_rows(collect_metrics(lm_fit3) %>% mutate(model = "all vars")) %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(caption = "RMSE and R-squared metrics for the linear regression models with more predictors",
               digits = 4)

```

```{r variablesinlinear}

#Let's collect the most important features for the model with all variables
#fit model with the entire training data
lm_fit <- workflow() %>%
  add_model(lm_spec) %>% 
  add_recipe(recipe_all) %>% 
  fit(df_train)
  
#10 Most statistically significant predictors
lm_fit %>% tidy() %>% 
  filter(p.value < 0.05) %>% 
  arrange(p.value, desc(abs(estimate))) %>% 
  head(6) %>% 
  knitr::kable(caption = "Model terms with the smallest p-values")
```


### Adding regularization to the model

While fitting the least squares estimates for the linear regression model, we did not observe signs of overfitting. However, it might be possible to drop some of the predictors and make the model easier to interpret. On the other hand, selecting the appropriate subset of predictors is not necessarily an easy task. Therefore, we will try to apply regularization to shrink the regression coefficients towards zero with a penalty parameter $\lambda$. We will use ridge regression and the lasso as shrinkage methods, and see if we can improve model performance.

In ridge regression, the regression coefficients are estimated by minimizing the following quantity:

$$
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \text{,}
$$

where the first term is the residual sum of squares (RSS), which we are minimizing when applying the least squares method for normal linear regression, and $\lambda  \geq 0$ is a regularization parameter which is tuned [@ISLRV2]. Figure \@ref(fig:ridge) shows the RMSE and R-squared values for 10-fold cross-validation. We notice that as the prediction error increases along with larger regularization parameter ($\lambda$) values. With the largest values the RMSE reaches the same level as the null model baseline prediction error.

```{r ridge, fig.cap="Rigde regression performance metrics calculated with 10-fold cross-validation for different values of the regularization parameter.", fig.height=3.5}
#Ridge regression

#specification
ridge_spec <- linear_reg(mixture = 0, penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

#ridge recipe
ridge_recipe <- 
  recipe(formula = rt ~ ., data = df_train) %>% 
  update_role(pubchem, MF, new_role = "ID") %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors()) #ridge is scale sensitive, hence the normalization of vars

#ridge workflow
ridge_workflow <- workflow() %>% 
  add_recipe(ridge_recipe) %>% 
  add_model(ridge_spec)

#tuning parameter values
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)

## Fit 10-fold CV to find best value for regularization
#Note: the fitting will give a warning if/when the regularization eliminates all predictors
tune_res <- tune_grid(
  ridge_workflow,
  resamples = train_folds, 
  grid = penalty_grid
)

#plot metrics
autoplot(tune_res)

```

The best performance in terms of RMSE is obtained with the smallest $\lambda$-value. This indicates that the amount of regularization is the smallest possible, and the model is very close to the linear regression model with all variables. The results of 10-fold cross-validation for this model are shown in Table \@ref(tab:bestridge), and we notice that the performance is not improved in comparison to linear regression.


```{r bestridge}

#best RMSE
best_penalty <- select_best(tune_res, metric = "rmse")
#best_penalty

#Performance with the best regularization parameter value
#Not as good as linear regression
tune_res %>% 
  collect_metrics() %>% 
  filter(penalty == best_penalty$penalty) %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 5, caption = "RMSE and R-squared metrics for the best performing ridge regression model")

```


Lasso is an alternative to ridge regression. In lasso the quantity to minimize is as follows:

$$
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} | \beta_j | \text{,}
$$

where again the first term is the RSS, but the following regularization term at the end is different from ridge regression. Results for 10-fold cross-validation are shown in Figure \@ref(fig:lasso). We see the same behavior as with ridge regression, where the best performance is obtained with the smallest amount of regularization. As the penalty term increases the coefficients shrink to zero and we essentially get the same level of performance as with the null model.

```{r lasso, fig.cap="Lasso performance metrics calculated with 10-fold cross-validation for different values of the regularization parameter.", fig.height=3.5}
# Lasso regression

#Lasso recipe
lasso_recipe <- ridge_recipe

#Model specs
lasso_spec <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

#Lasso workflow
lasso_workflow <- workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(lasso_spec)

#Fit 10-fold cv to tune penalty parameter
#Note: the fitting will give a warning if/when the regularization eliminates all predictors
tune_res <- tune_grid(
  lasso_workflow,
  resamples = train_folds, 
  grid = penalty_grid
)

autoplot(tune_res)

```

With lasso as well, the best performance in terms of RMSE is obtained with a small $\lambda$-value. The results of 10-fold cross-validation for the best lasso model are shown in Table \@ref(tab:bestlasso). The performance is very close to linear regression, which is not a surprize considering the small value of the penalty parameter. Regularization does not seem to improve the prediction performance of the linear regression model. This makes sense as the amount of observations is large compared to the amount of predictors.

```{r bestlasso}
#best RMSE
best_penalty <- select_best(tune_res, metric = "rmse")

#Performance with the best regularization parameter value
#Not as good as linear regression
tune_res %>% 
  collect_metrics() %>% 
  filter(penalty == best_penalty$penalty) %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 4, caption = "RMSE and R-squared metrics for the best performing lasso model")

```


### Polynomial regression

Regularization did not help us improve our model. Let's see if adding polynomial terms to the linear regression model helps us. We keep all variables and additionally raise the predictors related to physico-chemical properties (namely `r phys_chem`) to the second and third power. These new second and third order polynomial terms will be added to the model. Table \@ref(tab:polynomial) shows the metrics for this model determined through 10-fold cross validation. We notice that this helps us improve slightly on both metrics. However the improvement is not a massive one.

```{r polynomial}
#recipe for polynomial regression 
rec_poly <- recipe(rt ~ ., data = df_train) %>%
  update_role(MF, pubchem, new_role = "ID") %>% 
  step_poly(all_of(phys_chem), degree = 3) # include terms up to third power for terms related to phys_chem properties

# workflow for polynomial regression fit
poly_wf <- 
  workflow() %>%
  add_model(lm_spec) %>% #specification for linear regression
  add_recipe(rec_poly)

# Fit polynomial regression with 10-fold CV
poly_fit <- 
  poly_wf %>% 
  fit_resamples(resamples = train_folds)

poly_fit %>% 
  collect_metrics() %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 4, caption = "RMSE and R-squared metrics for the cubic regression model")
```

### Regression trees

Above, we tested several linear regression models with different tweaks. Next we will see if tree-based methods could bring some benefits. We start with a simple regression tree model.

Table \@ref(tab:treemodel) shows the RMSE and R-squared for a regression tree model, which was evaluated on the training data using 10-fold cross-validation. The initial results in terms of the metrics is not competitive compared to the best models we created earlier.

```{r treemodel}
reg_tree_spec <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

reg_tree_wf <- workflow() %>% 
  add_recipe(recipe_all) %>% 
  add_model(reg_tree_spec)

set.seed(2134)

# Fit regression trees with 10-fold CV
reg_tree_fit <- 
  reg_tree_wf %>% 
  fit_resamples(resamples = train_folds)

reg_tree_fit %>% 
  collect_metrics() %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 4, caption = "RMSE and R-squared metrics for a regression tree model")
```

We can try to improve model performance my tuning the cost complexity parameter. The tuning results are shown in Figure \@ref(fig:costcomplexity). The smallest parameter value seems to yield the best results. 

```{r costcomplexity, fig.cap="Tuning results for cost complexity parameter.", fig.height= 3.5}
#Let's try tuning cost complexity

reg_tree_wf <- workflow() %>%
  add_recipe(recipe_all) %>% 
  add_model(reg_tree_spec %>% set_args(cost_complexity = tune()))

param_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 5)

#Find best value for cost_compexity
tune_res <- tune_grid(
  reg_tree_wf, 
  resamples = train_folds, 
  grid = param_grid
)

#Plot tuning results
autoplot(tune_res)

#best model
best_tree <- select_best(tune_res, metric = "rmse")
```

The performance metrics for the model with the best cost complexity parameter are shown in Table \@ref(tab:besttreemetrics). By tuning the cost complexity parameter we were able to get the best prediction results thus far.

```{r besttreemetrics}
tune_res %>% 
  collect_metrics() %>% 
  filter(cost_complexity == best_tree$cost_complexity) %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 4, caption = "RMSE and R-squared metrics for the best performing regression tree model")
```


The downside of fitting a regression tree on a dataset with this many predictors is that the tree structure becomes so large and complex that visualization of all splits is not very helpful anymore. However, what we can visualize are the most important variables. This in fact will help us increase our understanding regarding which features affect the `rt` the most. 

For visualizing the variable importance, we first train the best performing tree model again with the entire training data. The most important variables are now shown in Figure \@ref(fig:variableimportance). Again we see that `ALogP` is connected to retention time. Aromatic rings, along with a few other variables, seem to be important as well.

```{r variableimportance, fig.cap="Most important variables in the regression tree model.", fig.height=3}
#Best model
reg_tree_final <- finalize_workflow(reg_tree_wf, best_tree)

#Fit tree on entire train data to visualize variable importance
reg_tree_final_fit <- fit(reg_tree_final, data = df_train)

#plot variable importance
reg_tree_final_fit %>%
     extract_fit_parsnip() %>%
     vip()
```

Although our tree model above outperformed the other models, there are a few disadvantages in using such a model. One issue concerning tree-based models is the fact that they can be very non-robust. To rephrase, small changes in data can lead to large changes in the final tree structure [@ISLRV2]. There are several methods to overcome this problem. Here we will apply boosting.

In boosting we first fit a very small tree to the data. In some cases the tree size can be as small as a single split. Next step is to fit another small tree to the residuals. This new tree will be added to the previous tree to improve model fit. Additionally a shrinkage parameter is applied to the trees to slow down the learning rate. After recursively fitting several small trees to the updated residuals, the final model can be expressed with the following equation:

$$
\hat{f} (x) = \sum_{b=1}^{B} \lambda \hat{f}^b(x) \text{,}
$$
where $B$ is the total amount of small trees in the model, $\lambda$ is the shrinkage parameter, and $\hat{f}^b(x)$ is the $b$th small tree in the model.

Let's fit a boosted tree model with 1000 trees which have a tree depth of two splits. The results of 10-fold cross-validation are shown in Table \@ref(tab:initialboosting). The boosted tree model outperforms all other models by a clear margin. Let's try to tune the model parameters to further improve the performance.


```{r initialboosting}
#Boost specification
boost_spec <- boost_tree(trees = 1000, tree_depth = 2) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_wf <- workflow() %>% 
  add_recipe(recipe_all) %>% 
  add_model(boost_spec)

set.seed(2134)

# Fit boosted trees with 10-fold CV
boost_fit <- 
  boost_wf %>% 
  fit_resamples(resamples = train_folds)

boost_fit %>% 
  collect_metrics() %>% 
  dplyr::select(-".config") %>% 
  knitr::kable(digits = 4, caption = "RMSE and R-squared metrics for a boosted tree model containing 1000 trees")
```

### Tuning the boosted tree model

The boosted tree model contains many parameters for us to tune, such as `tree_depth`, `learn_rate`, and `trees` among a few other parameters. This makes a standard grid search very inefficient and time consuming if we try to tune all possible parameters. There are different ways for tackling this problem. For example, the *finetune* package contains functions such as `tune_race_anova()`, which terminates training of non-promising parameter combinations early [@TMWR]. This can lead to significantly shorter execution times for the code. Here we will however restrict ourselves to tuning two of the parameters, namely `trees` and `tree_depth`, in a more traditional fashion.

As our first step for tuning, we set the tuning grid values for `tree_depth` as `r 1:4`, and for `trees` as `r c(500,1000,1500)`. The RMSE of models trained with these different parameter values show that 500 trees do not guarantee sufficient performance compared to larger models. We also notice that increasing `tree_depth` might further improve the results. Therefore, we add two more parameter combinations to our grid search, namely 1000 and 1500 `trees` with a `tree_depth` of 5. The tuning results via 10-fold cross-validation are shown in Figure \@ref(fig:tuningboosted). The best RMSE results are obtained with a `tree_depth` value of 5.

```{r tuningboosted, fig.cap="Tuning results for different number of trees and tree depth in the model. RMSE was calculated using 10-fold cross-validation.", fig.height=3}
# Un-comment the lines below if you want to run the tuning
# Results saved and loaded from csv-file to save time on knitting the document

# #Let's try to tune the model parameters
# ### Let's try tuning with grid search only two parameters -------------------------
# #Boost specification
# boost_spec <- boost_tree(trees = tune(), 
#                          tree_depth = tune()
#                          ) %>%
#   set_engine("xgboost") %>%
#   set_mode("regression")
# 
# #parameters to be tuned
# boost_parameters <- parameters(boost_spec)
# 
# #Tuning range
# boost_parameters %>% pull_dials_object("trees")
# boost_parameters %>% pull_dials_object("tree_depth")
# 
# #Let's use a smaller tree depth
# boost_parameters <- boost_parameters %>% update(tree_depth = tree_depth(c(1,4)))
# #Let's update the number of trees
# boost_parameters <- boost_parameters %>% update(trees = trees(c(500,1500)))
# 
# boost_parameters %>% pull_dials_object("tree_depth")
# boost_parameters %>% pull_dials_object("trees")
# # A regular grid with three levels for trees and 4 for tree_depth
# boost_grid <- grid_regular(boost_parameters, levels = c(trees = 3, tree_depth = 4))
# 
# #workflow for assessing the grid
# boost_wf <- 
#   workflow() %>% 
#   add_model(boost_spec) %>% 
#   add_recipe(recipe_all) 
#   
# set.seed(1234)
# ## grid search with rmse as the main metric
# # trees 500, 1000, 1500 & tree_depth 1, 2, 3, 4
# boost_tune <-
#   boost_wf %>%
#   tune_grid(
#     train_folds,
#     grid = boost_grid,
#     metrics = metric_set(rmse)
#   )
# 
# #save tuning results
# xgb_tuning_results <- collect_metrics(boost_tune)
# 
# #Let's see if tree depth of 5 improves results
# #Let's update tree depth
# boost_parameters <- boost_parameters %>% update(tree_depth = tree_depth(c(5,5)))
# #Let's update the number of trees
# boost_parameters <- boost_parameters %>% update(trees = trees(c(1000,1500)))
# 
# boost_parameters %>% pull_dials_object("tree_depth")
# boost_parameters %>% pull_dials_object("trees")
# # A regular grid with three levels for trees and 4 for tree_depth
# boost_grid2 <- grid_regular(boost_parameters, levels = c(trees = 2, tree_depth = 1))
# 
# set.seed(1234)
# ## grid search with rmse as the main metric
# # tree depth 5 and trees 1000 & 1500
# boost_tune2 <-
#   boost_wf %>%
#   tune_grid(
#     train_folds,
#     grid = boost_grid2,
#     metrics = metric_set(rmse)
#   )
# 
# #Let's combine the new results with the first round of tuning
# xgb_tuning_results_all <- 
#   xgb_tuning_results %>% 
#   full_join(collect_metrics(boost_tune2))
# 
# # Write results to a file
# write_csv(xgb_tuning_results_all, file = "regressionmetrics.csv")
# 

#Un-comment lines above to run model tuning code

#load tuning results from a csv-file
xgb_results <- read_csv(file = "regressionmetrics.csv")

#plotting the tuning results
xgb_results %>% 
  mutate(trees = as.factor(trees)) %>% 
  ggplot(aes(x = tree_depth, y = mean, color = trees)) +
  geom_line(aes(lty = trees)) +
  geom_point(size = 2) +
  theme(legend.position = "top") +
  labs(x = "Tree depth", y = "RMSE", color = "# Trees", lty = "# Trees")

```

The two best perfoming models are shown in Table \@ref(tab:bestboosts). The RMSE for the models with 1500 and 1000 trees is quite similar. Therefore, we will pick the simpler model with 1000 trees and tree depth of 5 as our final model. 

```{r bestboosts}
# Best models as table
#show_best(boost_tune) %>% select(-".estimator", -".config")
xgb_results %>% 
  dplyr::select(-".estimator", - ".config") %>% 
  arrange(mean) %>% 
  head(2) %>% 
  knitr::kable(digits = 4, caption = "RMSE for the two best models found during parameter tuning")
  
```


### Training and evaluating the final model

Let's train our final regression model using the entire training data. We will evaluate model performance on the test data. The results of the final model performance on the test set are shown in Table \@ref(tab:finaltestperformance).

```{r finaltestperformance}
#selecting the best
#select_best(boost_tune, metric = "rmse")

boost_parameters <- tibble(
  trees = 1000,
  tree_depth = 5
)

#updating the workflow with best parameters
final_boost_wf <- 
  boost_wf %>% 
  finalize_workflow(boost_parameters)

#Final regression model fit
final_boost_fit <- 
  final_boost_wf %>% 
  fit(df_train)

## Fit test data
df_test_preds <- augment(final_boost_fit, df_test)

## Metrics for test set
regression_metrics <- metric_set(rmse, rsq, mae)
regression_metrics(df_test_preds, truth = rt, estimate = .pred) %>% 
  knitr::kable(digits = 4, caption = "RMSE, R-squared, and MAE of the final model evaluated on the test data")

```

The model performs reasonably well on the test data. However, we are still not at the limit set by the mean experimental error of 36 seconds. This can be seen better as we visualize the residuals. Figure \@ref(fig:finalresiduals) shows that the residuals are evenly distributed around zero. Moreover, a large portion of the predictions are within the experimental error, although some predictions are quite far from the true value.

```{r finalresiduals, fig.cap="Residuals calculated for the final model are evenly distributed. The vertical dashed lines show the average experimental error of 36 s, determined in the original study.", fig.height=3}
## Visualizing residuals

df_test_preds %>% 
  dplyr::mutate(residuals = rt - .pred) %>% 
  ggplot(aes(x = residuals)) +
  geom_histogram(fill = "red", color = "black", binwidth = 10) +
  geom_vline(xintercept = c(-36, 36), lty = 2) #observed mean variability in retention time measurements

```

Table \@ref(tab:goodpredictions) shows the number and proportion of predictions with an absolute error smaller than 36 seconds. We see that around third of the predictions are within the experimental error limits.

```{r goodpredictions}
#how_many are good predictions
df_test_preds %>% 
  mutate(residuals = rt - .pred) %>% 
  mutate(good_pred = abs(residuals) < 36) %>% 
  group_by(good_pred) %>% 
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n)) %>% 
  knitr::kable(digits = 3, caption = "Number and proportion of predictions within and outside 36 s of the true value")
```

Finally, Figure \@ref(fig:predvstrue) visualizes the true `rt` values versus the predictions. We see that in general the observations stack around the line depicting perfect prediction. Additionally, the predictions seem to hold quite well for large and small values of `rt` alike.


```{r predvstrue, fig.cap="True retention time values versus the predicted values show fairly good correlation.", fig.height=3}
# ## Visualizing predictions to true values
# mod_perf.pl <- 
#   df_test_preds %>%
#   ggplot(aes(x = rt, y = .pred)) +
#   geom_point(alpha = 0.2) +
#   geom_abline(slope = 1) +
#   labs(x = "True values (s)", y = "Predicted values (s)")
# 
# ggsave("finalmodelplot.png", plot = mod_perf.pl, dpi = 200, device = "png", width = 5, height = 3, units = "in")

#plot saved as a png-file to make the final pdf-report file lighter

knitr::include_graphics(path = "finalmodelplot.png")
```


\newpage

# Conclusions

Our initial aim was to classify small molecules into non-retained and retained groups. Moreover, we wanted to predict the retention time for the retained molecules passing through the LC column using a simple set of molecular descriptors.

Logistic regression and random forest models were most effective in the classification task. Even though the large imbalance between the classes made the task difficult, our final model was able to beat the baseline performance of the naive classifier model. The logarithm of the partition coefficient `ALogP`, the sum of the atomic polarizabilities `apol`, along with the amount of carboxylic acid groups and esters were identified as important variables for classification.

The basic linear regression model faired quite well in predicting retention time. However, in the end we were able to improve model performance by using a boosted tree model. `ALogP` popped up several times when important variables for different models were assessed. The final model was reasonably good at predicting retention time, but there is clearly still room for improvement. More detailed tuning of the boosted tree model would most likely improve model performance even further. However, it is quite probable that the set of predictors we used for training our models does not contain all essential molecular information needed for accurately predicting retention time. Better model performance would no doubt be obtained by using more sophisticated molecular fingerprints as predictors.

In summary, it was shown that simple molecular descriptors can be used for building fairly good models for retention time prediction.

\newpage

# References